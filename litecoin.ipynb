{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary packages\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import gzip\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convert_json(json_str):\n",
    "#     converts json string to df\n",
    "    return json_normalize(json.loads(json_str))\n",
    "\n",
    "def convert_data(prefix, file_num):\n",
    "    for i in range(file_num):\n",
    "        suffix = str(i).zfill(12)\n",
    "        filename = prefix + suffix\n",
    "        \n",
    "        with gzip.open(filename, \"rt\", encoding = \"utf-8\") as file:\n",
    "            with mp.Pool() as pool:\n",
    "                results = pool.map(convert_json, [line for line in file if line])\n",
    "                mode = 'a' if i else 'w'\n",
    "                pd.concat(results, sort=False).to_csv('data.csv', header = not i, mode = mode)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert gunzipped json files from google bigquery to a single csv\n",
    "# convert_data('data/2019_01_04_', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df['outputs_addresses'] = df['outputs_addresses'].apply(\n",
    "        lambda addr: addr[2:-2])\n",
    "    df['inputs_addresses'] = df['inputs_addresses'].apply(\n",
    "        lambda addr: addr[2:-2])\n",
    "    return df\n",
    "df = load_data('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Address Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def construct_clusters(df):\n",
    "    clusters = []\n",
    "\n",
    "    def cluster(group):\n",
    "        addrs = group['inputs_addresses']\n",
    "        new_clstr = True\n",
    "        for idx, cluster in enumerate(clusters):\n",
    "            for addr in addrs:\n",
    "                if addr in cluster:\n",
    "                    clusters[idx].update(addrs)\n",
    "                    new_clstr = False\n",
    "                    break\n",
    "            if not new_clstr:\n",
    "                break\n",
    "        if new_clstr:\n",
    "            clusters.append(set(addrs))\n",
    "\n",
    "    start = time.time()\n",
    "    df.groupby('tx_hash').progress_apply(cluster)\n",
    "    end = time.time()\n",
    "    print(f'This operation took {round((end-start) / 3600, 2)} hr.')\n",
    "    return clusters\n",
    "# clusters = construct_clusters(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklify(obj, filename):\n",
    "# save obj in a pickle file for later\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "# picklify(clusters, 'clusters.pickle')\n",
    "\n",
    "def unpicklify(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "clusters = unpicklify('clusters.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_are_consolidated(clusters):\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for other in clusters[idx + 1:]:\n",
    "            inter = cluster.intersection(other)\n",
    "            if inter:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# def consolidate_clusters(clusters):\n",
    "#     consolidated = clusters_are_consolidated(clusters)\n",
    "    \n",
    "#     while()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
